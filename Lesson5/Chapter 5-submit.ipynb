{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear programming to solve an objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.optimize as spopt\n",
    "\n",
    "c = [10,30]\n",
    "A = [[2,-1]]\n",
    "b = [0]\n",
    "r1_bound = (10,None)\n",
    "r7_bound = (10,None)\n",
    "res = spopt.linprog(c,A_ub=A,b_ub=b, \n",
    "                    bounds=(r1_bound,r7_bound), method='simplex')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.optimize as spopt\n",
    "\n",
    "c = [-0.03,-0.02,-0.001,-0.001]\n",
    "A = [[1,1,0,0],[0,0,2,1]]\n",
    "b = [20000, 30000]\n",
    "x1_bound = (0,2*10**6)\n",
    "x2_bound = (0,4*10**6)\n",
    "x3_bound = (0,2*10**6)\n",
    "x4_bound = (0,4*10**6)\n",
    "res = spopt.linprog(c,A_ub=A,b_ub=b, \n",
    "                          bounds=(x1_bound,x2_bound,x3_bound,x4_bound), \n",
    "                    method='simplex')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finite difference examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def finite_diff(f,x):\n",
    "    dx = 1e-10\n",
    "    return((f(x+dx)-f(x-dx))/(2*dx))\n",
    "\n",
    "f = lambda x: x**2\n",
    "\n",
    "finite_diff(f,4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def finite_diff(f,x,y):\n",
    "    dx = dy = 1e-10\n",
    "    return( (f(x+dx,y+dy)-f(x,y))/((dx+dy)/2) )\n",
    "\n",
    "f = lambda x, y: y**2 + 10*(x/y)\n",
    "\n",
    "finite_diff(f,20,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def finite_diff_xy(f, x, y, dx, dy):\n",
    "    \n",
    "    return( (f(x+dx,y+dy)-f(x-dx,y-dy))/(2*(dx+dy)) )\n",
    "\n",
    "f = lambda x, y: y**2 + (10*(x/y))\n",
    "\n",
    "finite_diff_xy(f,20,10,0,1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def finite_diff_xy(f, x, y, dx, dy):\n",
    "    return( (f(x+dx,y+dy)-f(x-dx,y-dy))/(2*(dx+dy)) )\n",
    "\n",
    "f1 = lambda x, y: y**2 + (10*(x/y))\n",
    "f2 = lambda x, y: 10*x + 30*y\n",
    "\n",
    "J = np.array([[finite_diff_xy(f1,20,10,1e-10,0),finite_diff_xy(f1,20,10,0,1e-10)],\n",
    "              [finite_diff_xy(f2,20,10,1e-10,0),finite_diff_xy(f2,20,10,0,1e-10)]])\n",
    "\n",
    "J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "x = np.maximum(25,np.abs(np.arange(-100,100,0.1)))**2\n",
    "ax = sns.lineplot(np.arange(-100,100,0.1),x)\n",
    "ax.set(xlabel=\"x\",ylabel=\"function\")\n",
    "plt.savefig('multiple_minima.png',dpi=300)\n",
    "\n",
    "plt.figure()\n",
    "x = np.abs(np.arange(-100,100,0.1))**2\n",
    "ax = sns.lineplot(np.arange(-100,100,0.1),x)\n",
    "ax.set(xlabel=\"x\",ylabel=\"function\")\n",
    "plt.savefig('single_minima.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.arange(-100,100,0.5)\n",
    "y = np.arange(-100,100,0.5)\n",
    "xx,yy = np.meshgrid(x,y)\n",
    "z = xx**2 - yy**2\n",
    "fig, ax = plt.subplots()\n",
    "cx = plt.contourf(xx,yy,z)\n",
    "ax.set(xlabel=\"x\",ylabel=\"y\")\n",
    "fig.colorbar(cx)\n",
    "plt.savefig('saddle_points.png',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization of an objective\n",
    "\n",
    "1. Grid Search\n",
    "2. Gradient descent\n",
    "3. Momentum based gradient descent\n",
    "4. Newtons method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.arange(-50,50,10)\n",
    "y = np.arange(-50,50,10)\n",
    "xx,yy = np.meshgrid(x,y)\n",
    "z = np.ones(xx.shape)\n",
    "plt.scatter(xx,yy)\n",
    "plt.xlim(-100,100)\n",
    "plt.ylim(-100,100)\n",
    "plt.savefig('gridsearch-2.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(R1,R7):\n",
    "    if R1<10 or R7 <10 or (2*R1-R7)<=0:\n",
    "        return(np.inf)\n",
    "    else:\n",
    "        return(10*R1+30*R7)\n",
    "\n",
    "min_val = np.inf\n",
    "# search the grid of values between -50 and 50\n",
    "# with 100 points for each parameter\n",
    "for R1 in range(-50,50):\n",
    "    for R7 in range(-50,50):\n",
    "        if min_val > cost(R1,R7):\n",
    "            param = (R1,R7)\n",
    "            min_val = cost(R1,R7)\n",
    "\n",
    "print(param, min_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utility functions for visualization\n",
    "def plot_contour(support, f, levels):\n",
    "    x = np.arange(-support,support,0.05)\n",
    "    y = np.arange(-support,support,0.05)\n",
    "    xx,yy = np.meshgrid(x,y)\n",
    "    vecs = np.stack((xx,yy))\n",
    "    z = f(vecs) \n",
    "    logz = np.log(z)\n",
    "    fig, ax = plt.subplots(figsize=(12,10))\n",
    "    cx = plt.contour(xx,yy,logz, levels=levels, cmap='tab10')\n",
    "    ax.set_xlabel(\"x\", fontsize=32)\n",
    "    ax.set_ylabel(\"y\", fontsize=32)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "    plt.xlim(-support,support)\n",
    "    plt.ylim(-support,support)\n",
    "    cbar = fig.colorbar(cx)\n",
    "    cbar.ax.tick_params(labelsize=16)\n",
    "    return(fig)\n",
    "    \n",
    "def add_grads(fnum, x_updates, grads, scale, color):\n",
    "#     fig = plt.figure(fnum)\n",
    "    q = plt.quiver(x_updates[:-1,0],x_updates[:-1,1],-grads[1:,0],-grads[1:,1], scale=scale, width=0.005, \n",
    "               alpha=0.5, color=color)\n",
    "    return(q)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# cost function\n",
    "def cost(R1,R7):\n",
    "    return(R1**2+R7**2)\n",
    "\n",
    "def plot_cost(x):\n",
    "    return(x[0]**2+x[1]**2)\n",
    "\n",
    "\n",
    "# finite difference approximation\n",
    "def finite_diff_xy(f, x, y, dx, dy):\n",
    "    return( (f(x+dx,y+dy)-f(x-dx,y-dy))/(2*(dx+dy)) )\n",
    "\n",
    "# gradient calculation using finite difference\n",
    "def gradient(f, x):\n",
    "    dx = dy = 1e-10\n",
    "    return(np.array([ finite_diff_xy(f,x[0],x[1],dx,0), \n",
    "                      finite_diff_xy(f,x[0],x[1],0,dy)]) )\n",
    "\n",
    "# gradient descent to find minimum value\n",
    "def gradient_descent(gradient, f, x, eta):\n",
    "    x_prev = np.array(x)\n",
    "    x_new = np.zeros(x_prev.shape)\n",
    "    # learning rate/step size\n",
    "    eta = eta\n",
    "    # store the x values and gradients at each step in x_updates and grads\n",
    "    x_updates = x_prev\n",
    "    grads = np.zeros(x_prev.shape)\n",
    "    \n",
    "    # initalize tolerance and threshold for termination\n",
    "    tolerance = 1e-10\n",
    "    threshold = 1\n",
    "\n",
    "    # loop until threshold is under tolerance\n",
    "    while(threshold > tolerance):\n",
    "        # calculate gradient\n",
    "        grad = gradient(f, x_prev)\n",
    "        # take a step in opposite direction of gradient\n",
    "        x_new = x_prev - (eta*grad)\n",
    "        # calcualte threshold and update the parameters\n",
    "        threshold = np.linalg.norm(x_prev-x_new)\n",
    "        x_prev = x_new\n",
    "        # store the gradients and updated parameters\n",
    "        grads = np.vstack((grads,grad))\n",
    "        x_updates = np.vstack((x_updates,x_prev))\n",
    "    return(x_new,x_updates, grads)\n",
    "\n",
    "x_new, x_updates, grads = gradient_descent(gradient, cost, np.array([4,6]), 0.1)\n",
    "\n",
    "# plot the function and gradients to reach minimum\n",
    "support = 10\n",
    "levels = 100\n",
    "fig = plot_contour(support, plot_cost, levels)\n",
    "grad = add_grads(fig.number, x_updates, grads, 200, 'red')\n",
    "\n",
    "plt.savefig('gradient_descent.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def cost(R1,R7):\n",
    "    return(R1**2+R7**2)\n",
    "\n",
    "def plot_cost(x):\n",
    "    return(x[0]**2+x[1]**2)\n",
    "\n",
    "\n",
    "def finite_diff_xy(f, x, y, dx, dy):\n",
    "    return( (f(x+dx,y+dy)-f(x-dx,y-dy))/(2*(dx+dy)) )\n",
    "\n",
    "def gradient(f, x):\n",
    "    dx = dy = 1e-10\n",
    "    return(np.array([ finite_diff_xy(f,x[0],x[1],dx,0), \n",
    "                      finite_diff_xy(f,x[0],x[1],0,dy)]) )\n",
    "\n",
    "# start at (4,6)\n",
    "def gradient_descent(gradient, f, x, eta):\n",
    "    x_prev = np.array(x)\n",
    "    x_new = np.zeros(x_prev.shape)\n",
    "    # learning rate/step size\n",
    "    eta = eta\n",
    "    x_updates = x_prev\n",
    "    grads = np.zeros(x_prev.shape)\n",
    "    tolerance = 1e-10\n",
    "    threshold = 1\n",
    "\n",
    "    while(threshold > tolerance):\n",
    "        grad = gradient(f, x_prev)\n",
    "\n",
    "        x_new = x_prev - (eta*grad)\n",
    "        threshold = np.linalg.norm(x_prev-x_new)\n",
    "        x_prev = x_new\n",
    "        # store the gradients and updated parameters\n",
    "        grads = np.vstack((grads,grad))\n",
    "        x_updates = np.vstack((x_updates,x_prev))\n",
    "    return(x_new,x_updates, grads)\n",
    "\n",
    "x_new, x_updates, grads = gradient_descent(gradient, cost, np.array([4,6]), 0.1)\n",
    "\n",
    "support = 10\n",
    "levels = 100\n",
    "fig = plot_contour(support, plot_cost, levels)\n",
    "grad = add_grads(fig.number, x_updates[0:2,:], -grads[0:2,:], 200, 'red')\n",
    "grad = add_grads(fig.number, x_updates[0:2,:], grads[0:2,:], 200, 'blue')\n",
    "\n",
    "plt.savefig('gradient_descent_direction.png',dpi=300)\n",
    "x_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## momentum illustration using the booth function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# cost function - booth function\n",
    "def cost(R1,R7):\n",
    "    return(np.power(R1+2*R7-7,2)+np.power(2*R1+R7-5,2))\n",
    "\n",
    "def plot_cost(x):\n",
    "    return(np.power(x[0]+2*x[1]-7,2)+np.power(2*x[0]+x[1]-5,2))\n",
    "\n",
    "\n",
    "# finite difference approximation\n",
    "def finite_diff_xy(f, x, y, dx, dy):\n",
    "    return( (f(x+dx,y+dy)-f(x-dx,y-dy))/(2*(dx+dy)) )\n",
    "\n",
    "# gradient calculation using finite difference\n",
    "def gradient(f, x):\n",
    "    dx = dy = 1e-10\n",
    "    return(np.array([ finite_diff_xy(f,x[0],x[1],dx,0), \n",
    "                      finite_diff_xy(f,x[0],x[1],0,dy)]) )\n",
    "\n",
    "# gradient descent to find minimum value\n",
    "def gradient_descent(gradient, f, x, eta):\n",
    "    x_prev = np.array(x)\n",
    "    x_new = np.zeros(x_prev.shape)\n",
    "    # learning rate/step size\n",
    "    eta = eta\n",
    "    # store the x values and gradients at each step in x_updates and grads\n",
    "    x_updates = x_prev\n",
    "    grads = np.zeros(x_prev.shape)\n",
    "    \n",
    "    # initalize tolerance and threshold for termination\n",
    "    tolerance = 1e-10\n",
    "    threshold = 1\n",
    "\n",
    "    # loop until threshold is under tolerance\n",
    "    while(threshold > tolerance):\n",
    "        # calculate gradient\n",
    "        grad = gradient(f, x_prev)\n",
    "        # take a step in opposite direction of gradient\n",
    "        x_new = x_prev - (eta*grad)\n",
    "        # calcualte threshold and update the parameters\n",
    "        threshold = np.linalg.norm(x_prev-x_new)\n",
    "        x_prev = x_new\n",
    "        # store the gradients and updated parameters\n",
    "        grads = np.vstack((grads,grad))\n",
    "        x_updates = np.vstack((x_updates,x_prev))\n",
    "    return(x_new,x_updates, grads)\n",
    "\n",
    "\n",
    "\n",
    "def momentum_gradient(gradient, f, x, eta, alpha):\n",
    "    x_prev = np.array(x)\n",
    "    x_new = np.zeros(x_prev.shape)\n",
    "    # learning rate/step size\n",
    "    eta = eta\n",
    "    # momentum\n",
    "    alpha = alpha\n",
    "    # store the x values and gradients at each step in x_updates and grads\n",
    "    x_updates = x_prev\n",
    "    grads = np.zeros(x_prev.shape)\n",
    "    \n",
    "    # initalize tolerance and threshold for termination\n",
    "    tolerance = 1e-10\n",
    "    threshold = 1\n",
    "\n",
    "    # loop until threshold is under tolerance\n",
    "    while(threshold > tolerance):\n",
    "        # calculate gradient\n",
    "        grad = gradient(f, x_prev)\n",
    "        # take a step in opposite direction of gradient\n",
    "        if len(grads.shape) > 1:\n",
    "            grad_momentum = (eta*grad) + (alpha*grads[-1,:])\n",
    "        else:\n",
    "            grad_momentum = (eta*grad)\n",
    "        x_new = x_prev - grad_momentum\n",
    "        # calcualte threshold and update the parameters\n",
    "        threshold = np.linalg.norm(x_prev-x_new)\n",
    "        x_prev = x_new\n",
    "        # store the gradients and updated parameters\n",
    "        grads = np.vstack((grads,grad_momentum))\n",
    "        x_updates = np.vstack((x_updates,x_prev))\n",
    "    return(x_new,x_updates, grads)\n",
    "\n",
    "x_new, x_updates, grads = gradient_descent(gradient, cost, np.array([10,0]), 0.1)\n",
    "x_new, momentum_updates, momentum_grads = momentum_gradient(gradient, cost, np.array([10,0]), 0.1, 0.2)\n",
    "\n",
    "print(grads.shape, momentum_grads.shape)\n",
    "\n",
    "# plot the function and gradients to reach minimum\n",
    "support = 10\n",
    "levels = 100\n",
    "fig = plot_contour(support, plot_cost, levels)\n",
    "grad = add_grads(fig.number, x_updates, grads, 200, 'red')\n",
    "momentum = add_grads(fig.number, momentum_updates, momentum_grads, 20, 'blue')\n",
    "\n",
    "plt.legend((grad,momentum),('Gradient','Momentum'),prop={'size': 16})\n",
    "plt.savefig('gradient_descent_momentum.png',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "f = lambda x : np.power(x[0]**2+x[1]-11,2)+np.power(x[0]+x[1]**2-7,2)\n",
    "support = 7\n",
    "fig = plot_contour(support, f, 140)\n",
    "plt.show(fig)\n",
    "plt.savefig('himmelbaus.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "\n",
    "# cost function - booth function\n",
    "def cost(x):\n",
    "    return(np.power(x[0]+2*x[1]-7,2)+np.power(2*x[0]+x[1]-5,2))\n",
    "\n",
    "grad_f = grad(cost)\n",
    "\n",
    "\n",
    "def gradient_descent(gradient, x, eta):\n",
    "    x_prev = np.array(x)\n",
    "    x_new = np.zeros(x_prev.shape)\n",
    "    # learning rate/step size\n",
    "    eta = eta\n",
    "    x_updates = x_prev\n",
    "    grads = np.zeros(x_prev.shape)\n",
    "    tolerance = 1e-10\n",
    "    threshold = 1\n",
    "    iteration = 1\n",
    "    \n",
    "    while(threshold > tolerance):\n",
    "        grad = gradient(x_prev)\n",
    "        # decrease step size as iteration go up\n",
    "        x_new = x_prev - (eta*grad)\n",
    "        threshold = np.linalg.norm(x_prev-x_new)\n",
    "        x_prev = x_new\n",
    "        # store the gradients and updates\n",
    "        grads = np.vstack((grads,grad))\n",
    "        x_updates = np.vstack((x_updates,x_prev))\n",
    "        iteration += 1\n",
    "        if iteration%1000==0:\n",
    "            print(\"gradient iteration:\",iteration)\n",
    "    return(x_new,x_updates, grads)\n",
    "\n",
    "from autograd import hessian\n",
    "hess_f = hessian(cost)\n",
    "\n",
    "def newtons_method(gradient, hessian, x, eta):\n",
    "    x_prev = np.array(x)\n",
    "    x_new = np.zeros(x_prev.shape)\n",
    "    # learning rate/step size\n",
    "    eta = eta\n",
    "    x_updates = x_prev\n",
    "    steps = np.zeros(x_prev.shape)\n",
    "    tolerance = 1e-10\n",
    "    threshold = 1\n",
    "    iteration = 1\n",
    "    \n",
    "    while(threshold > tolerance):\n",
    "        grad = gradient(x_prev)\n",
    "        hess = hessian(x_prev)\n",
    "        # calculate step direction\n",
    "        step = np.matmul(np.linalg.inv(hess), grad)\n",
    "        # decrease step size\n",
    "        x_new = x_prev - (eta*step)\n",
    "        threshold = np.linalg.norm(x_prev-x_new)\n",
    "        x_prev = x_new\n",
    "        # store the gradients and updated parameters\n",
    "        steps = np.vstack((steps,step))\n",
    "        x_updates = np.vstack((x_updates,x_prev))\n",
    "        iteration += 1\n",
    "        if iteration%1000==0:\n",
    "            print(\"newton iteration:\",iteration)\n",
    "    return(x_new,x_updates, steps)\n",
    "\n",
    "\n",
    "support = 12\n",
    "fig = plot_contour(support, cost, 50)\n",
    "\n",
    "x_new_grad, x_updates, grads = gradient_descent(grad_f, np.array([10.0,-10.0]), 0.1)\n",
    "x_new, newton_updates, steps = newtons_method(grad_f,hess_f,np.array([10.0,-10.0]), 0.1)\n",
    "print(\"gradient_steps=\", x_updates.shape, x_new_grad)\n",
    "print(\"newton_steps=\", newton_updates.shape, x_new)\n",
    "gradient_2 = add_grads(fig.number, x_updates, grads, 200, 'green')\n",
    "newton_2 = add_grads(fig.number, newton_updates, steps, 200, 'cyan')\n",
    "\n",
    "\n",
    "x_new_grad, x_updates, grads = gradient_descent(grad_f, np.array([10.0,-10.0]), 0.2)\n",
    "x_new, newton_updates, steps = newtons_method(grad_f,hess_f,np.array([10.0,-10.0]), 0.2)\n",
    "print(\"gradient_steps=\",x_updates.shape, x_new_grad)\n",
    "print(\"newton_steps=\",newton_updates.shape, x_new)\n",
    "gradient = add_grads(fig.number, x_updates, grads, 200, 'red')\n",
    "newton = add_grads(fig.number, newton_updates, 1*steps, 200, 'blue')\n",
    "\n",
    "\n",
    "plt.legend((gradient,newton,gradient_2,newton_2),('Gradient(eta=0.2)','newton(eta=0.2)',\n",
    "                                                  'gradient(eta=0.1)','newton(eta=0.1)'),prop={'size': 16})\n",
    "plt.savefig('newtons_method.png',dpi=300)\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## automatic differentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from autograd import grad\n",
    "from autograd import numpy as np\n",
    "# cost function\n",
    "def cost(x):\n",
    "    return(x[1]**2+(10*x[0]/x[1]))\n",
    "\n",
    "gradient = grad(cost)\n",
    "gradient_val = gradient(np.array([20.0,10.0]))\n",
    "print(gradient_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dill function for activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "def fun(x):\n",
    "    import numpy as np\n",
    "    np.random.seed(100)\n",
    "    val = np.power( (np.power(x[0],2) + x[1]-11) ,2) + np.power( (x[0]+ np.power(x[1],2)-7) ,2) #+ np.random.normal(loc=-1.0,scale=5)\n",
    "    return(val)\n",
    "\n",
    "file = open('function_activity1','wb')\n",
    "dill.dump(fun,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import dill\n",
    "# from inspect import signature\n",
    "# function = open('function_activity1','rb')\n",
    "\n",
    "# # load function using dill package\n",
    "# f = dill.load(function)\n",
    "# sig = signature(f)\n",
    "# str(sig)\n",
    "# f([10,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "from autograd import hessian\n",
    "import dill\n",
    "%matplotlib inline\n",
    "\n",
    "function = open('function_activity1','rb')\n",
    "\n",
    "# load function using dill package\n",
    "f = dill.load(function)\n",
    "\n",
    "# calculate gradient using autograd\n",
    "grad_f = grad(f)\n",
    "\n",
    "# perform gradient descent \n",
    "def gradient_descent(gradient, x, eta):\n",
    "    x_prev = np.array(x)\n",
    "    x_new = np.zeros(x_prev.shape)\n",
    "    # learning rate/step size\n",
    "    eta = eta\n",
    "    x_updates = x_prev\n",
    "    grads = np.zeros(x_prev.shape)\n",
    "    tolerance = 1e-10\n",
    "    threshold = 1\n",
    "    iteration = 1\n",
    "    \n",
    "    while(threshold > tolerance):\n",
    "        grad = gradient(x_prev)\n",
    "        # decrease step size as iteration go up\n",
    "        x_new = x_prev - (eta*grad)\n",
    "        threshold = np.linalg.norm(x_prev-x_new)\n",
    "        x_prev = x_new\n",
    "        # store the gradients and updates\n",
    "        grads = np.vstack((grads,grad))\n",
    "        x_updates = np.vstack((x_updates,x_prev))\n",
    "        iteration += 1\n",
    "    return(x_new,x_updates, grads)\n",
    "\n",
    "x1_new, x_updates, grads = gradient_descent(grad_f, np.array([-10.0,10.0]),0.001)\n",
    "print(\"Search terminated\")\n",
    "support = 10\n",
    "print(\"Best Parameters from gradient descent:\",x1_new)\n",
    "print(\"steps for gradient descent\",x_updates.shape[0])\n",
    "\n",
    "fig = plot_contour(support, f, 100)\n",
    "grad = add_grads(fig.number, x_updates, grads, 2*10**4, 'red')\n",
    "plt.savefig('gradient_descent_auto.png',dpi=300)\n",
    "plt.show(fig)\n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.arange(-support,support,0.05)\n",
    "y = np.arange(-support,support,0.05)\n",
    "xx,yy = np.meshgrid(x,y)\n",
    "vecs = np.stack((xx,yy))\n",
    "z = f(vecs) \n",
    "logz = np.log(z)\n",
    "np.min(logz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "from autograd import hessian\n",
    "import dill\n",
    "%matplotlib inline\n",
    "\n",
    "function = open('function_activity1','rb')\n",
    "f = dill.load(function)\n",
    "\n",
    "grad_f = grad(f)\n",
    "hess_f = hessian(f)\n",
    "\n",
    "def gradient_descent(gradient, x, eta):\n",
    "    x_prev = np.array(x)\n",
    "    x_new = np.zeros(x_prev.shape)\n",
    "    # learning rate/step size\n",
    "    eta = eta\n",
    "    x_updates = x_prev\n",
    "    grads = np.zeros(x_prev.shape)\n",
    "    tolerance = 1e-10\n",
    "    threshold = 1\n",
    "    iteration = 1\n",
    "    \n",
    "    while(threshold > tolerance):\n",
    "        grad = gradient(x_prev)\n",
    "        # decrease step size as iteration go up\n",
    "        x_new = x_prev - (eta*grad)\n",
    "        threshold = np.linalg.norm(x_prev-x_new)\n",
    "        x_prev = x_new\n",
    "        # store the gradients and updates\n",
    "        grads = np.vstack((grads,grad))\n",
    "        x_updates = np.vstack((x_updates,x_prev))\n",
    "        iteration += 1\n",
    "        if iteration%1000==0:\n",
    "            print(\"newton iteration:\",iteration)\n",
    "    return(x_new,x_updates, grads)\n",
    "\n",
    "x1_new, x_updates, grads = gradient_descent(grad_f, np.array([-10.0,10.0]),0.001)\n",
    "x2_new, newton_updates, steps = newtons_method(grad_f,hess_f,np.array([-10.0,10.0]), 0.8)\n",
    "print(\"terminated\")\n",
    "support = 7\n",
    "print(\"Best Parameters from gradient descent:\",x1_new)\n",
    "print(\"Best Parameters from newtons method:\",x2_new)\n",
    "\n",
    "print(\"steps for gradient descent\",x_updates.shape[0])\n",
    "print(\"steps for newtons method\", newton_updates.shape[0])\n",
    "fig = plot_contour(support, f, 140)\n",
    "grad = add_grads(fig.number, x_updates, grads, 10000, 'red')\n",
    "newton = add_grads(fig.number, newton_updates, steps, 50, 'blue')\n",
    "plt.show(fig)\n",
    "plt.savefig('newtown_auto.png',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "data = np.random.normal(1000, 100,1000) + np.random.normal(100,20,1000)\n",
    "\n",
    "sns.distplot(data, kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import fmin\n",
    "%matplotlib inline\n",
    "\n",
    "def cost(x):\n",
    "    return(np.power(x[0]+2*x[1]-7,2)+np.power(2*x[0]+x[1]-5,2))\n",
    "\n",
    "# fmin uses nelder-mead but has the option to return all vectors for analysis unlike minimize\n",
    "\"\"\"\n",
    "res_min = scipy.optimize.minimize(cost,np.array([-10,10]), method='Nelder-Mead')\n",
    "\n",
    "Optimization terminated successfully.\n",
    "         Current function value: 0.000000\n",
    "         Iterations: 49\n",
    "         Function evaluations: 94\n",
    " final_simplex: (array([[0.99997756, 3.00002429],\n",
    "       [1.00003059, 3.00000105],\n",
    "       [1.00006341, 2.99995153]]), array([1.10730863e-09, 4.94047525e-09, 7.26423839e-09]))\n",
    "           fun: 1.1073086283232063e-09\n",
    "       message: 'Optimization terminated successfully.'\n",
    "          nfev: 94\n",
    "           nit: 49\n",
    "        status: 0\n",
    "       success: True\n",
    "             x: array([0.99997756, 3.00002429])\n",
    "Optimization terminated successfully.\n",
    "         Current function value: 0.000000\n",
    "         Iterations: 49\n",
    "         Function evaluations: 94\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "result = fmin(cost, np.array([-10,10]), retall=True)\n",
    "print(result[0])\n",
    "res = np.array(result[1])\n",
    "support = 15\n",
    "fig = plot_contour(support, cost, 140)\n",
    "grad = add_grads(fig.number, res, 3*np.sign(res), 100, 'red')\n",
    "plt.savefig('Nelder-mead.png',dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import differential_evolution\n",
    "import dill\n",
    "%matplotlib inline\n",
    "\n",
    "function = open('function_activity1','rb')\n",
    "f = dill.load(function)\n",
    "\n",
    "def cost(x):\n",
    "    return(np.power(x[0]+2*x[1]-7,2)+np.power(2*x[0]+x[1]-5,2))\n",
    "\n",
    "bounds = [(-12,12),(-12,12)]\n",
    "result = differential_evolution(f, bounds, popsize=5, mutation=(0.5,0.8))\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# sns.scatterplot(data[:,0],data[:,1],sizes=[1000])\n",
    "\n",
    "# utility functions for visualization\n",
    "def plot_contourf(support, f, levels):\n",
    "    x = np.arange(support[0],support[1],0.05)\n",
    "    y = np.arange(support[0],support[1],0.05)\n",
    "    xx,yy = np.meshgrid(x,y)\n",
    "    vecs = np.stack((xx,yy))\n",
    "    z = f(vecs) \n",
    "    #logz = np.abs(np.log(z))\n",
    "    logz = np.log(z)\n",
    "    fig, ax = plt.subplots(figsize=(12,10))\n",
    "    cx = plt.contourf(xx,yy,logz, levels=levels, cmap='tab10')\n",
    "    ax.set_xlabel(\"x\", fontsize=32)\n",
    "    ax.set_ylabel(\"y\", fontsize=32)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "    plt.xlim(support[0],support[1])\n",
    "    plt.ylim(support[0],support[1])\n",
    "    cbar = fig.colorbar(cx)\n",
    "    cbar.ax.tick_params(labelsize=16)\n",
    "    return(fig)\n",
    "\n",
    "def f(x):\n",
    "    result = np.power((x[0]-10),2)+np.power((x[1]-10),2)\n",
    "    return(result)\n",
    "\n",
    "support = [0,20]\n",
    "f = plot_contourf(support,f,4)\n",
    "\n",
    "np.random.seed(10)\n",
    "cov1 = np.array(([0.1,0],[0,0.1]))\n",
    "mean1 = np.array([13,13])\n",
    "data = np.random.multivariate_normal(mean1, cov1,10)\n",
    "np.min(data)\n",
    "plt.scatter(data[:,0],data[:,1],color='b', alpha=0.5, linewidths=[5])\n",
    "plt.savefig('high-low.png',dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimensions: (150, 4)\n",
      "Training set dimensions: (100, 4)\n",
      "Testing set dimensions: (50, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "print(\"Dataset dimensions:\",iris.data.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.33, random_state=100)\n",
    "print(\"Training set dimensions:\",X_train.shape)\n",
    "print(\"Testing set dimensions:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimensions: (150, 4)\n",
      "1.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "2.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "3.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "4.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "5.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "6.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "7.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "8.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "9.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "10.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "11.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "12.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "13.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "14.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "15.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "16.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "17.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "18.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "19.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "20.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "21.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "22.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "23.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "24.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "25.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "26.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "27.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "28.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "29.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "30.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "31.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "32.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "33.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "34.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "35.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "36.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "37.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "38.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "39.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "40.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "41.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "42.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "43.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "44.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "45.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "46.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "47.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "48.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "49.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "50.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "51.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "52.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "53.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "54.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "55.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "56.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "57.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "58.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "59.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "60.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "61.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "62.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "63.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "64.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "65.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "66.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "67.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "68.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "69.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "70.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "71.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "72.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "73.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "74.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "75.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "76.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "77.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "78.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "79.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "80.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "81.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "82.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "83.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "84.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "85.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "86.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "87.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "88.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "89.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "90.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "91.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "92.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "93.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "94.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "95.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "96.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "97.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "98.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "99.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "100.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "101.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "102.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "103.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "104.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "105.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "106.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "107.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "108.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "109.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "110.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "111.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "112.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "113.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "114.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "115.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "116.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "117.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "118.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "119.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "120.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "121.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "122.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "123.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "124.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "125.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "126.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "127.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "128.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "129.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "130.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "131.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "132.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "133.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "134.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "135.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "136.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "137.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "138.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "139.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "140.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "141.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "142.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "143.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "144.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "145.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "146.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "147.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "148.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "149.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n",
      "150.\tTraining set:(149,) \n",
      "\tTesting set:(1,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "print(\"Dataset dimensions:\",iris.data.shape)\n",
    "loo = LeaveOneOut()\n",
    "count = 1\n",
    "for train_index, test_index in loo.split(iris.data):\n",
    "    print(\"{2}.\\tTraining set:{0} \\n\\tTesting set:{1}\".format(train_index.shape,test_index.shape,count))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimensions: (150, 4)\n",
      "\n",
      "K-Fold cross validation\n",
      "\n",
      "1.\tTraining set:(100,) \n",
      "\tTesting set:(50,)\n",
      "2.\tTraining set:(100,) \n",
      "\tTesting set:(50,)\n",
      "3.\tTraining set:(100,) \n",
      "\tTesting set:(50,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "print(\"Dataset dimensions:\",iris.data.shape)\n",
    "kf = KFold(n_splits=3)\n",
    "kf.get_n_splits(iris.data)\n",
    "count = 1\n",
    "print(\"\\nK-Fold cross validation\\n\")\n",
    "for train_index, test_index in kf.split(iris.data):\n",
    "    print(\"{2}.\\tTraining set:{0} \\n\\tTesting set:{1}\".format(train_index.shape,test_index.shape,count))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimensions: (150, 4)\n",
      "\n",
      "K-Fold cross validation\n",
      "\n",
      "1.\tTraining set:Counter({1: 50, 2: 50}) \n",
      "\tTesting set:Counter({0: 50})\n",
      "2.\tTraining set:Counter({0: 50, 2: 50}) \n",
      "\tTesting set:Counter({1: 50})\n",
      "3.\tTraining set:Counter({0: 50, 1: 50}) \n",
      "\tTesting set:Counter({2: 50})\n",
      "\n",
      "Stratified K-Fold cross validation\n",
      "\n",
      "1.\tTraining set:Counter({0: 33, 1: 33, 2: 33}) \n",
      "\tTesting set:Counter({0: 17, 1: 17, 2: 17})\n",
      "2.\tTraining set:Counter({0: 33, 1: 33, 2: 33}) \n",
      "\tTesting set:Counter({0: 17, 1: 17, 2: 17})\n",
      "3.\tTraining set:Counter({0: 34, 1: 34, 2: 34}) \n",
      "\tTesting set:Counter({0: 16, 1: 16, 2: 16})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import datasets\n",
    "from collections import Counter\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "print(\"Dataset dimensions:\",iris.data.shape)\n",
    "\n",
    "print(\"\\nK-Fold cross validation\\n\")\n",
    "kf = KFold(n_splits=3)\n",
    "kf.get_n_splits(iris.data)\n",
    "count = 1\n",
    "for train_index, test_index in kf.split(iris.data):\n",
    "    train_count = Counter(iris.target[train_index])\n",
    "    test_count = Counter(iris.target[test_index])\n",
    "    print(\"{2}.\\tTraining set:{0} \\n\\tTesting set:{1}\".format(train_count,test_count,count))\n",
    "    count += 1\n",
    "\n",
    "print(\"\\nStratified K-Fold cross validation\\n\")\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "skf.get_n_splits(iris.data,iris.target)\n",
    "count = 1    \n",
    "for train_index, test_index in skf.split(iris.data, iris.target):\n",
    "    train_count = Counter(iris.target[train_index])\n",
    "    test_count = Counter(iris.target[test_index])\n",
    "    print(\"{2}.\\tTraining set:{0} \\n\\tTesting set:{1}\".format(train_count,test_count,count))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'max_depth': 4, 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'random_state': 1}\n",
      "Best score from cross validation: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X_train = iris.data\n",
    "y_train = iris.target\n",
    "parameters = {\"criterion\": [\"gini\",\"entropy\"],\n",
    "              \"min_samples_split\": [item for item in range(2,10,2)],\n",
    "              \"max_depth\": [item for item in range(2,10,2)],\n",
    "              \"min_samples_leaf\": [item for item in range(1,5)],\n",
    "              \"max_leaf_nodes\": [None] + [item for item in range(2,10,2)],\n",
    "              \"random_state\" : [1]\n",
    "              }\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "gridsearch_dt = GridSearchCV(dt, parameters, cv=5)\n",
    "gridsearch_dt.fit(X_train,y_train)\n",
    "print(gridsearch_dt.best_params_)\n",
    "print(\"Best score from cross validation: {0}\".format(gridsearch_dt.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity\n",
    "\n",
    "Use cross validation to find optimal depth for decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiran/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:517: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "/home/kiran/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:517: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "df = pd.read_csv('~/Dropbox/packt/snakes_edit.csv')\n",
    "\n",
    "# transform data to design matrix\n",
    "X_total = df.drop(['Venom_ind','Venom'],axis=1)\n",
    "y = df['Venom_ind']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_total, y, test_size=0.33, random_state=100)\n",
    "\n",
    "# scale numeric data to have zero mean and unit variance\n",
    "X_scaled_temp = X_train.loc[:,['Length','Diameter','Rattle','Food','Predator','Scales Per Inch']]\n",
    "scaler = StandardScaler().fit(X_scaled_temp)\n",
    "X_scaled_temp = scaler.transform(X_scaled_temp)\n",
    "X_train.loc[:,['Length','Diameter','Rattle','Food','Predator','Scales Per Inch']] = X_scaled_temp\n",
    "\n",
    "X_scaled_test = X_test.loc[:,['Length','Diameter','Rattle','Food','Predator','Scales Per Inch']]\n",
    "X_scaled_temp = scaler.transform(X_scaled_test)\n",
    "X_test.loc[:,['Length','Diameter','Rattle','Food','Predator','Scales Per Inch']] = X_scaled_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6818181818181818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=100,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=4,\n",
    "                                random_state=100)\n",
    "dt.fit(X_train, y_train)\n",
    "print(dt.score(X_test,y_test))\n",
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dtree-before.png'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot decision tree using graphviz\n",
    "from sklearn import tree\n",
    "import graphviz\n",
    "dot_data = tree.export_graphviz(dt, out_file=None)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.format = 'png'\n",
    "graph.render(\"dtree-before\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'max_depth': 8, 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 4, 'random_state': 100}\n",
      "0.6794029850746268\n"
     ]
    }
   ],
   "source": [
    "parameters = {\"criterion\": [\"gini\"],\n",
    "              \"min_samples_split\": [item for item in range(2,10,2)],\n",
    "              \"max_depth\": [item for item in range(2,10,2)],\n",
    "              \"min_samples_leaf\": [item for item in range(1,5)],\n",
    "              \"max_leaf_nodes\": [None] + [item for item in range(2,10,2)],\n",
    "              \"random_state\" : [100]\n",
    "              }\n",
    "dt = DecisionTreeClassifier()\n",
    "gridsearch_dt = GridSearchCV(dt, parameters, cv=5)\n",
    "gridsearch_dt.fit(X_train,y_train)\n",
    "print(gridsearch_dt.best_score_)\n",
    "print(gridsearch_dt.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-- Testing best parameters [Grid]...\n",
      "0.6845454545454546\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=8,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=4,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=100,\n",
      "            splitter='best')\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n-- Testing best parameters [Grid]...\")\n",
    "dt_gridsearch = gridsearch_dt.best_estimator_\n",
    "dt_gridsearch.fit(X_train, y_train)\n",
    "print(dt_gridsearch.score(X_test,y_test))\n",
    "print(dt_gridsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dtree-afterCV.png'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot decision tree using graphviz\n",
    "from sklearn import tree\n",
    "import graphviz\n",
    "dot_data = tree.export_graphviz(dt_gridsearch, out_file=None)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.format = 'png'\n",
    "graph.render(\"dtree-afterCV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
